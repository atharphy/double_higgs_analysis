{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ad469f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.style.use('ggplot')\n",
    "import time\n",
    "import uproot\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from pprint import pprint\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import warnings\n",
    "#need ROOT\n",
    "import ROOT\n",
    "warnings.filterwarnings(\n",
    "    'ignore', category=pd.io.pytables.PerformanceWarning)\n",
    "\n",
    "input_vars = ['pxB1', 'pyB1', 'pzB1', 'eB1', 'pxB2', 'pyB2', 'pzB2', 'eB2', 'pxJ1', 'pyJ1', 'pzJ1', 'eJ1',\n",
    "              'pxJ2', 'pyJ2', 'pzJ2', 'eJ2', 'pxL1', 'pyL1', 'pzL1', 'eL1', 'pxN1', 'pyN1', 'pzN1', 'eN1',\n",
    "              'pxH', 'pyH', 'pzH', 'eH', 'pxt11', 'pyt11', 'pzt11', 'et11', 'pxt12', 'pyt12', 'pzt12',\n",
    "              'et12', 'pxt21', 'pyt21', 'pzt21', 'et21', 'pxt22', 'pyt22', 'pzt22', 'et22', 'pxW1', 'pyW1', \n",
    "              'pzW1', 'eW1', 'pxW2', 'pyW2', 'pzW2', 'eW2', 'm_B1', 'pt_B1', 'eta_B1', 'phi_B1', 'm_B2', \n",
    "              'pt_B2', 'eta_B2', 'phi_B2', 'm_J1', 'pt_J1', 'eta_J1', 'phi_J1', 'm_J2', 'pt_J2', 'eta_J2', \n",
    "              'phi_J2', 'm_L1', 'pt_L1', 'eta_L1', 'phi_L1', 'm_N1', 'pt_N1', 'eta_N1', 'phi_N1', 'm_H', \n",
    "              'pt_H', 'eta_H', 'phi_H', 'm_t11', 'pt_t11', 'eta_t11', 'phi_t11', 'm_t12', 'pt_t12', \n",
    "              'eta_t12', 'phi_t12', 'm_t21', 'pt_t21', 'eta_t21', 'phi_t21', 'm_t22', 'pt_t22', 'eta_t22', \n",
    "              'phi_t22', 'm_W1', 'pt_W1', 'eta_W1', 'phi_W1', 'm_W2', 'pt_W2', 'eta_W2', 'phi_W2'\n",
    "             ]\n",
    "def get_columns(fname):\n",
    "    \n",
    "    return columns, todrop\n",
    "\n",
    "\n",
    "def build_filelist(input_dir):\n",
    "    files = [ifile for ifile in glob('{}/*.csv'.format(input_dir))]\n",
    "    nominal = {\n",
    "        'hh': [], \n",
    "        'ttbar': []\n",
    "    }\n",
    "    systematics = {}\n",
    "    for fname in files:\n",
    "        print(fname)\n",
    "        if 'hh' in fname:\n",
    "            nominal['hh'].append(fname)\n",
    "        elif 'tt' in fname:\n",
    "            nominal['ttbar'].append(fname)\n",
    "    return nominal, systematics\n",
    "\n",
    "def process_files(all_data, files, is_signal):\n",
    "    for ifile in files:\n",
    "        print (ifile)\n",
    "        MakeCsvDataFrame = ROOT.RDF.MakeCsvDataFrame\n",
    "        input_file = MakeCsvDataFrame(ifile)\n",
    "        print('This is the file ' + ifile) \n",
    "        input_dict = input_file.AsNumpy()\n",
    "        input_df = pd.DataFrame.from_dict(input_dict)\n",
    "        print(\"Nevents = \", input_df.shape[0])\n",
    "        slim_df = input_df[input_vars]\n",
    "        single_meta_df = pd.DataFrame(slim_df.index,columns = ['index'])\n",
    "        single_meta_df['names'] = np.full(len(slim_df), ifile)\n",
    "        single_meta_df['isSignal'] = np.ones(len(slim_df)) if is_signal == 1 else np.zeros(len(slim_df))\n",
    "        single_training_df = slim_df.astype('float64')\n",
    "        single_training_df['isSM'] = np.zeros(len(single_meta_df)) if is_signal == -1.0 else np.ones(len(single_meta_df))\n",
    "        all_data['meta'] = pd.concat([all_data['meta'], single_meta_df])\n",
    "        all_data['train'] = pd.concat([all_data['train'], single_training_df])\n",
    "            \n",
    "    return all_data\n",
    "\n",
    "def build_scaler(sm_only):\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(sm_only.values)\n",
    "    scaler_info = pd.DataFrame.from_dict({\n",
    "        'mean': scaler.mean_,\n",
    "        'scale': scaler.scale_,\n",
    "        'variance': scaler.var_,\n",
    "        'nsamples': scaler.n_samples_seen_\n",
    "    })\n",
    "    scaler_info.set_index(sm_only.columns.values, inplace=True)\n",
    "    return scaler, scaler_info\n",
    "\n",
    "def format_for_store(all_data, scaler):\n",
    "    formatted_data = pd.DataFrame(\n",
    "        scaler.transform(all_data['train'].values),\n",
    "        columns=all_data['train'].columns.values, dtype='float64'\n",
    "    )\n",
    "    print(formatted_data)\n",
    "    formatted_data['idx'] = all_data['meta']['index'].values\n",
    "    formatted_data['sample_names'] = all_data['meta']['names'].values\n",
    "    formatted_data['signal'] = all_data['meta']['isSignal'].values\n",
    "    return formatted_data\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "store = pd.HDFStore('/localdata/Athar/datasets/{}.h5'.format('preprocessed_dataset'),\n",
    "                    complevel=9, complib='bzip2')\n",
    "all_data = {\n",
    "    'meta': pd.DataFrame(),\n",
    "    'train': pd.DataFrame()\n",
    "}\n",
    "filelist, _ = build_filelist('/localdata/Athar/input_files')  # list of files to process\n",
    "all_data = process_files(all_data, filelist['hh'], is_signal = 1)\n",
    "all_data = process_files(all_data, filelist['ttbar'], is_signal = 0)\n",
    "sm_only = all_data['train'][(all_data['train']['isSM'] == 1)]\n",
    "scaler, store['scaler'] = build_scaler(sm_only)\n",
    "store['nominal'] = format_for_store(all_data, scaler)\n",
    "print(store['nominal'])\n",
    "print ('Complete! Preprocessing completed in {} seconds'.format(time.time() - start))\n",
    "store.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
